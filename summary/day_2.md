# Day 2, pages 16-26

## Summary (요약)
Rich history of RL, has 3 big branches
1. Trial & error
    * Law of effect: "The greater the satisfaction or discomfort, the greater the strengthening or weakening of the bond"
2. Optimal control
    * Problem of designing a controller to minimize a measure of a dynamical system's behavior over time
    * Led to the development of:
      * Bellman equation
      * Dynamic programming
      * Markovial decision processes (MDP); a discrete stochastic version
      * Learning + dynamic programming = Heuristic dynamic programming
3. Temporal difference
    * A stimulus that has been paired with a primary enforcer such as food or pain, and as a result have similar reinforcing properties.

Actor-critic: User of temporal-difference in trial-and-error learning


## Impressions (인상깊은점)
Many algorithms in RL have been inspired by nature (how animals learn, etc.)
Lots of collaboration led to advancement in the field


## Questions (의문점)
In what ways can we collaborate with other fields (subfield of RL or completely different field such as computer vision, unsupervised learning, non-CS fields)
to advance RL?
